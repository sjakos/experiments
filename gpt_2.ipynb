{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjakos/experiments/blob/develop/gpt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GA3oRLxMRohz",
        "colab_type": "code",
        "outputId": "96d46e3d-6e8f-493a-d596-1f2b37c70568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "cell_type": "code",
      "source": [
        "!cd /content && rm -rf gpt-2 && git clone https://github.com/lucidrains/gpt-2.git\n",
        "%cd /content/gpt-2\n",
        "!pip install -r requirements.txt\n",
        "!mkdir -p models/117M && sh download_model.sh 117M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 35 (delta 16), reused 28 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n",
            "/content/gpt-2\n",
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Requirement already satisfied: tensorflow>=1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.13.0rc1)\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.14.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.12.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (3.6.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.13.0rc0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (0.32.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12->-r requirements.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12->-r requirements.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.12->-r requirements.txt (line 2)) (40.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow>=1.12->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.12->-r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow>=1.12->-r requirements.txt (line 2)) (5.1.2)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5\n",
            "mkdir: cannot create directory ‘models/117M’: File exists\n",
            "Copying gs://gpt-2/models/117M/checkpoint...\n",
            "/ [1 files][   77.0 B/   77.0 B]                                                \n",
            "Operation completed over 1 objects/77.0 B.                                       \n",
            "Copying gs://gpt-2/models/117M/encoder.json...\n",
            "/ [1 files][ 1017 KiB/ 1017 KiB]                                                \n",
            "Operation completed over 1 objects/1017.9 KiB.                                   \n",
            "Copying gs://gpt-2/models/117M/hparams.json...\n",
            "/ [1 files][   90.0 B/   90.0 B]                                                \n",
            "Operation completed over 1 objects/90.0 B.                                       \n",
            "Copying gs://gpt-2/models/117M/model.ckpt.data-00000-of-00001...\n",
            "\n",
            "Operation completed over 1 objects/474.7 MiB.                                    \n",
            "Copying gs://gpt-2/models/117M/model.ckpt.index...\n",
            "/ [1 files][  5.1 KiB/  5.1 KiB]                                                \n",
            "Operation completed over 1 objects/5.1 KiB.                                      \n",
            "Copying gs://gpt-2/models/117M/model.ckpt.meta...\n",
            "/ [1 files][460.1 KiB/460.1 KiB]                                                \n",
            "Operation completed over 1 objects/460.1 KiB.                                    \n",
            "Copying gs://gpt-2/models/117M/vocab.bpe...\n",
            "/ [1 files][445.6 KiB/445.6 KiB]                                                \n",
            "Operation completed over 1 objects/445.6 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VOLJcvV4X7GG",
        "colab_type": "code",
        "outputId": "0c5ecf18-bbca-4c1e-ed69-8b012eb547f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        }
      },
      "cell_type": "code",
      "source": [
        "!python src/interactive_conditional_samples.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-02-14 23:52:30.273921: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-02-14 23:52:30.274299: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2b90c00 executing computations on platform Host. Devices:\n",
            "2019-02-14 23:52:30.274336: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-02-14 23:52:30.433240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-02-14 23:52:30.433781: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2b8ef20 executing computations on platform CUDA. Devices:\n",
            "2019-02-14 23:52:30.433815: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-02-14 23:52:30.434200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-02-14 23:52:30.434243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-02-14 23:52:31.442771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-02-14 23:52:31.442831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-02-14 23:52:31.442850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-02-14 23:52:31.443148: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-02-14 23:52:31.443242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> OpenAI, an artificial intelligence research group co-founded by billionaire Elon Musk, has demonstrated a piece of software that can produce authentic-looking fake news articles after being given just a few piece of information.\n",
            "2019-02-14 23:53:43.090304: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "AI, an artificial intelligence research group co-founded by billionaire Elon Musk, has demonstrated a piece of software that can produce authentic-looking fake news articles after being given just a few piece of information. However, by using WolfBrain, however, the researchers can automate the appearance of its news articles, via deployment and visualization software. Technically, the system can mimic the appearance of tones in the cartoon tweets message currently circulating on Twitter, in the same way that a \"Eye candy index\" serves as an index of valid brand research. NV Corp Aerotel eStam shows that even web-based bots can imitate the appearance of a website by using Twitter's logo on the page, indicating how it is anonymizing the contents before exertingatively modifying the artificial means. \"Given the fact that social media attracts a lot of jobs and are profitable to help companies keep up with technology trends, there are probably protections for employees,\" says Hydra, a researcher at CBS that is adding features for Northrop Grumman Stratfor, Intel, Mars and others including military, law enforcement and public buildings.\n",
            "\n",
            "Photograph by: SciFiFortress (CC BY-NC 2.0)<|endoftext|>Thanks to sndf10 from India and smd bayent, AMD reported an Isocition usage of 82-hit operations. This means that for AMD its 59-way clock upswing is strong in its chosen generation (319MHz) and maximum overclock of 99% and compare Intel's GTX 680 and R9 290X running at 105%. What seems surprising is a large correlation. The difference I saw is the 2011-2016 Ryzen 79-ATX and Ryzen 7 1700 on 27- and 36-NIC processors, which is slightly closer to AMD's 2015-Present (31.4- and 64.1.56MHz, respectively).\n",
            "\n",
            "AMD CP101 CR-D08KR Q7700K\n",
            "\n",
            "\n",
            "Introduction\n",
            "\n",
            "\n",
            "Let's look at the benchmark charts from AMD's current Xeon PC series (23nm) for some sort of optimization. I'll be presenting them as prime numbers as that's sheer perceived performance gains with their unified computing clusters up, for this standard benchmark. One of these numbers is 51-tick vector cores (WM 6300) which in total have a frequency of 27 MHz with a maximum frequency of 89 MHz each. Ryzen 7 1700 performs similarly in all three cases. Intel clearly cares just a little more about optimizing on these values (20 thread-measure latency boosts, 57.5% state VB-coherent debug stack open close, 28 thread-measure kick in, 97% dedicated instruction modes). The GPUs are designed to work with these three a lot as there are not many\n",
            "================================================================================\n",
            "Model prompt >>> "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}